{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Pytorch 教學](#)\n",
    "   [1.1 起手式:    建立5x3張量](#1.1起手式) <br>\n",
    "   [1.2 張量型態:  裡面的值改變型態](#1.2型態)<br>\n",
    "   [1.3 張量的運算:  Operations](#1.3運算)<br>\n",
    "   [1.4 張量取值: 當作list 選位置](#1.4取值)<br>\n",
    "   [1.5 重組張量: resize](#1.5重組)<br>\n",
    "   [1.6 取張量值: ](#1.6取張量值)<br>\n",
    "   [1.7 numpy 轉 tensor](#1.7numpy轉tensor)<br>\n",
    "   [1.8 CUDA Tensors](#1.8CUDATensors)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.1起手式](#)\n",
    "### 建立5x3張量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty tensor: tensor([[2.1667e+38, 4.1759e-43, 2.1667e+38],\n",
      "        [4.1759e-43, 2.1667e+38, 4.1759e-43],\n",
      "        [2.1667e+38, 4.1759e-43, 2.1667e+38],\n",
      "        [4.1759e-43, 2.1667e+38, 4.1759e-43],\n",
      "        [2.1667e+38, 4.1759e-43, 2.1667e+38]])\n",
      "\n",
      "random tensor: tensor([[0.7356, 0.2985, 0.6063],\n",
      "        [0.2129, 0.2363, 0.3170],\n",
      "        [0.0243, 0.0587, 0.2886],\n",
      "        [0.3172, 0.8356, 0.3389],\n",
      "        [0.2793, 0.4072, 0.3240]])\n",
      "\n",
      "zeros tensor: tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "\n",
      "with own data: tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_empty = torch.empty(5, 3)\n",
    "# 隨機\n",
    "x_rand = torch.rand(5, 3)\n",
    "# 零矩陣\n",
    "x_zeros = torch.zeros(5, 3, dtype=torch.long)\n",
    "# 可貸資料 (list)\n",
    "x_tensor = torch.tensor([5.5, 3])\n",
    "print(f\"empty tensor: {x_empty}\\n\\nrandom tensor: {x_rand}\\n\\nzeros tensor: {x_zeros}\\n\\n\\\n",
    "with own data: {x_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  [1.2型態](#)\n",
    "## 由存在的張量 override 為要的型態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一開始存在的張量: \n",
      "tensor([0, 1, 2])\n",
      "\n",
      "\n",
      "new_ones 方法:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "randn_like 改變型態方法:\n",
      " tensor([[ 1.7780, -0.3162, -0.0433],\n",
      "        [ 0.2625, -0.7015,  0.2371],\n",
      "        [ 0.0873,  0.5967, -0.0274],\n",
      "        [-1.4628,  0.3425,  0.1812],\n",
      "        [-1.6392, -0.1490,  0.3612]])\n",
      "\n",
      "\n",
      "改變型態後的張量size 一樣:\n",
      " torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# 假設有存在的張量 x_exist \n",
    "x_exist = torch.tensor([i for i in range(3)])\n",
    "print(f\"一開始存在的張量: \\n{x_exist}\\n\\n\")\n",
    "\n",
    "x_exist = x_exist.new_ones(5, 3, dtype = torch.double)\n",
    "print(f\"new_ones 方法:\\n {x_exist}\\n\\n\")\n",
    "\n",
    "x_exist = torch.randn_like(x, dtype = torch.float)\n",
    "print(f\"randn_like 改變型態方法:\\n {x_exist}\\n\\n\")\n",
    "print(f\"改變型態後的張量size 一樣:\\n {x_exist.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.3運算](#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5,3)\n",
    "y = torch.rand(5,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法一: 直接用 +\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1178, 0.8911, 1.4198],\n",
      "        [1.6518, 0.5261, 1.1395],\n",
      "        [1.6270, 1.5350, 0.9505],\n",
      "        [1.0196, 1.0519, 0.8512],\n",
      "        [0.8284, 0.8649, 1.8724]])\n"
     ]
    }
   ],
   "source": [
    "print(x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法二: torch.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1178, 0.8911, 1.4198],\n",
      "        [1.6518, 0.5261, 1.1395],\n",
      "        [1.6270, 1.5350, 0.9505],\n",
      "        [1.0196, 1.0519, 0.8512],\n",
      "        [0.8284, 0.8649, 1.8724]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法三 先給一個空張量 再帶入這個空張量\n",
    "#### 這個方法跟做list 時候一樣 先給空list 在append 進去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1178, 0.8911, 1.4198],\n",
      "        [1.6518, 0.5261, 1.1395],\n",
      "        [1.6270, 1.5350, 0.9505],\n",
      "        [1.0196, 1.0519, 0.8512],\n",
      "        [0.8284, 0.8649, 1.8724]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法四: 用存在的張量 .add => y.add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1178, 0.8911, 1.4198],\n",
      "        [1.6518, 0.5261, 1.1395],\n",
      "        [1.6270, 1.5350, 0.9505],\n",
      "        [1.0196, 1.0519, 0.8512],\n",
      "        [0.8284, 0.8649, 1.8724]])\n"
     ]
    }
   ],
   "source": [
    "print(y.add(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### [小筆記](#)\n",
    "##### 任何將張量就地改變的操作都用底線(\"_\") 固定。例如：x.copy_（y），x.t_（）將更改x。\n",
    "##### Any operation that mutates a tensor in-place is post-fixed with an _. For example: x.copy_(y), x.t_(), will change x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.4取值]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[0.5761, 0.0308, 0.9046],\n",
      "        [0.8161, 0.4526, 0.3660],\n",
      "        [0.9324, 0.9154, 0.6068],\n",
      "        [0.4383, 0.4440, 0.7304],\n",
      "        [0.5617, 0.2064, 0.9872]])\n",
      "\n",
      "取值 x[:, 1]:\n",
      "  tensor([0.0308, 0.4526, 0.9154, 0.4440, 0.2064])\n",
      "\n",
      "============================================================\n",
      "\n",
      "取值 x[:, 1:]:\n",
      "  tensor([[0.0308, 0.9046],\n",
      "        [0.4526, 0.3660],\n",
      "        [0.9154, 0.6068],\n",
      "        [0.4440, 0.7304],\n",
      "        [0.2064, 0.9872]])\n",
      "============================================================\n",
      "\n",
      "這邊注意到:\n",
      "        x[1], x[1:]\n",
      "的不同\n"
     ]
    }
   ],
   "source": [
    "print(f\"x:\\n {x}\")\n",
    "print()\n",
    "print(f\"取值 x[:, 1]:\\n  {x[:, 1]}\")\n",
    "print()\n",
    "print(\"============================================================\\n\")\n",
    "print(f\"取值 x[:, 1:]:\\n  {x[:, 1:]}\")\n",
    "print(\"============================================================\\n\")\n",
    "print(\"這邊注意到:\\n\\\n",
    "        x[1], x[1:]\\n的不同\\\n",
    "\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.5重組](#)\n",
    "### 利用view 重組本來的張量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2344,  0.3694, -0.5273,  1.0555],\n",
      "        [ 0.1619, -0.3552, -1.0573, -1.1506],\n",
      "        [-0.9016, -0.3744,  1.1431, -1.2303],\n",
      "        [-0.3460,  1.3223, -0.9379,  1.4431]])\n",
      "\n",
      "torch.Size([4, 4])\n",
      "\n",
      "tensor([ 0.2344,  0.3694, -0.5273,  1.0555,  0.1619, -0.3552, -1.0573, -1.1506,\n",
      "        -0.9016, -0.3744,  1.1431, -1.2303, -0.3460,  1.3223, -0.9379,  1.4431])\n",
      "\n",
      "torch.Size([16])\n",
      "\n",
      "tensor([[ 0.2344,  0.3694, -0.5273,  1.0555,  0.1619, -0.3552, -1.0573, -1.1506],\n",
      "        [-0.9016, -0.3744,  1.1431, -1.2303, -0.3460,  1.3223, -0.9379,  1.4431]])\n",
      "\n",
      "torch.Size([2, 8])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(f\"{x}\\n\\n{x.size()}\\n\\n{y}\\n\\n{y.size()}\\n\\n{z}\\n\\n{z.size()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.6取張量值](#)\n",
    "#### 請注意這邊只能取其中一個的值, 若要取全部用迴圈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23435992002487183\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(x[0,0].item())\n",
    "print(type(x[0,0].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.23435992002487183,\n",
       " 0.16191042959690094,\n",
       " -0.9016234278678894,\n",
       " -0.3459957540035248]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[i,0].item() for i in range(len(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy 方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.23435992002487183,\n",
       "  0.36935681104660034,\n",
       "  -0.5272515416145325,\n",
       "  1.055497407913208],\n",
       " [0.16191042959690094,\n",
       "  -0.35517945885658264,\n",
       "  -1.0573357343673706,\n",
       "  -1.1505929231643677],\n",
       " [-0.9016234278678894,\n",
       "  -0.374403715133667,\n",
       "  1.143140435218811,\n",
       "  -1.2302770614624023],\n",
       " [-0.3459957540035248,\n",
       "  1.3222938776016235,\n",
       "  -0.937863290309906,\n",
       "  1.4430803060531616]]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.7numpy轉tensor](#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5 6]\n",
      "tensor([2, 3, 4, 5, 6], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([i for i in range(1,6)])\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(f\"{a}\\n{b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.8CUDATensors](#)\n",
    "### 利用GPU 跑tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2344,  1.3694,  0.4727,  2.0555],\n",
      "        [ 1.1619,  0.6448, -0.0573, -0.1506],\n",
      "        [ 0.0984,  0.6256,  2.1431, -0.2303],\n",
      "        [ 0.6540,  2.3223,  0.0621,  2.4431]], device='cuda:0')\n",
      "tensor([[ 1.2344,  1.3694,  0.4727,  2.0555],\n",
      "        [ 1.1619,  0.6448, -0.0573, -0.1506],\n",
      "        [ 0.0984,  0.6256,  2.1431, -0.2303],\n",
      "        [ 0.6540,  2.3223,  0.0621,  2.4431]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python37064bitebb1a2532921419ca117c3e5b0bda326"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
